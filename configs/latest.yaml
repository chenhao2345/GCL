# general
beta1: 0                         # Adam hyperparameter
beta2: 0.999                     # Adam hyperparameter

gen_iters: 1                     # updating dis 5 times, updating gen 1 time
image_display_iter: 200         # How often do you want to display output images during training
snapshot_save_iter: 5000       # How often to save the checkpoint
step_size: 10                # when to decay the learning rate
log_iter: 100                     # How often do you want to log the training stats

lr_id: 0.00035                    # initial appearance encoder learning rate
lr_d: 0.0001                     # initial discriminator learning rate
lr_g: 0.0001                     # initial generator (except appearance encoder) learning rate
lr_policy: multistep             # learning rate scheduler [multistep|constant|step]
train_bn: true                   # whether we train the bn for the generated image.

display_size: 32                 # How much display images
gamma: 0.1                       # Learning Rate Decay (except appearance encoder)
gamma2: 0.1                      # Learning Rate Decay (for appearance encoder)
weight_decay: 0.0005             # weight decay

gan_w: 1.0                         # the weight of gan loss
recon_f_w: 5.0                    # the initial weight for appearance code reconstruction
recon_x_cyc_w: 5.0                 # the initial weight for cycle reconstruction
recon_x_w: 5.0                     # the initial weight for self-reconstruction
memory_id_w: 1.0                  # positive ID loss

# memory
momentum: 0.2                     # memory momentum alpha
temperature: 0.04                 # contrastive temperature
K: 8192                           # number of negative samples

input_dim: 1

# model
dis:              
  LAMBDA: 0.01                   # the hyperparameter for the regularization term
  activ: lrelu                   # activation function style [relu/lrelu/prelu/selu/tanh]
  dim: 32                        # number of filters in the bottommost layer
  gan_type: lsgan                # GAN loss [lsgan/nsgan]
  n_layer: 2                     # number of layers in D
  n_res: 4                       # number of layers in D
  non_local: 0                   # number of non_local layers
  norm: none                     # normalization layer [none/bn/in/ln]
  num_scales: 3                  # number of scales
  pad_type: reflect              # padding type [zero/reflect]

gen:
  activ: lrelu                   # activation function style [relu/lrelu/prelu/selu/tanh]
  dec: basic                     # [basic/parallel/series]
  dim: 16                        # number of filters in the bottommost layer
  dropout: 0                     # use dropout in the generator
  id_dim: 2048                   # length of appearance code
  mlp_dim: 512                   # number of filters in MLP
  mlp_norm: none                 # norm in mlp [none/bn/in/ln]
  n_downsample: 2                # number of downsampling layers in content encoder
  n_res: 4                       # number of residual blocks in content encoder/decoder
  non_local: 0                   # number of non_local layer
  pad_type: reflect              # padding type [zero/reflect]
  tanh: false                    # use tanh or not at the last layer
  init: kaiming                  # initialization [gaussian/kaiming/xavier/orthogonal]
